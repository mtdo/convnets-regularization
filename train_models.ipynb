{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print(\"Training set shape:\", x_train.shape)\n",
    "print(\"Test set shape:\", x_test.shape)\n",
    "labels = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\",\n",
    "          \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random image from training set\n",
    "img = np.random.randint(len(x_train))\n",
    "plt.title(labels[y_train[img][0]])\n",
    "plt.imshow(x_train[img])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scale RGB in test and train inputs\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "\n",
    "# Transform labels to one hot representation\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "y_test = to_categorical(y_test, num_classes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define optimizer and training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "opt = RMSprop(lr=0.001, decay=1e-6)\n",
    "\n",
    "# Training hyperparameters\n",
    "batch_size = 64\n",
    "epochs = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "# Base model\n",
    "b = Sequential()\n",
    "b.add(Conv2D(filters=9, kernel_size=(5,5), padding=\"same\", input_shape=(32,32,3),\n",
    "             activation=\"relu\"))\n",
    "b.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "b.add(Conv2D(filters=16, kernel_size=(5,5), padding=\"same\",\n",
    "             activation=\"relu\"))\n",
    "b.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "b.add(Flatten())\n",
    "b.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "# Base model\n",
    "b.compile(loss=\"categorical_crossentropy\",\n",
    "          optimizer=opt,\n",
    "          metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "# Base model\n",
    "b_history = b.fit(x_train, y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(x_test, y_test),\n",
    "                  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and training history\n",
    "# Base model\n",
    "b.save(\"models/b.h5\")\n",
    "with open(\"models/b_history\", \"wb\") as f:\n",
    "    pickle.dump(b_history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "# Enhanced model\n",
    "eb = Sequential()\n",
    "eb.add(Conv2D(filters=16, kernel_size=(5,5), padding=\"same\", input_shape=(32,32,3),\n",
    "              activation=\"relu\"))\n",
    "eb.add(Conv2D(filters=16, kernel_size=(5,5), padding=\"same\",\n",
    "              activation=\"relu\"))\n",
    "eb.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "eb.add(Conv2D(filters=32, kernel_size=(5,5), padding=\"same\",\n",
    "              activation=\"relu\"))\n",
    "eb.add(Conv2D(filters=32, kernel_size=(5,5), padding=\"same\",\n",
    "              activation=\"relu\"))\n",
    "eb.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "eb.add(Conv2D(filters=64, kernel_size=(5,5), padding=\"same\",\n",
    "              activation=\"relu\"))\n",
    "eb.add(Conv2D(filters=64, kernel_size=(5,5), padding=\"same\",\n",
    "              activation=\"relu\"))\n",
    "eb.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "eb.add(Flatten())\n",
    "eb.add(Dense(128, activation=\"relu\"))\n",
    "eb.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "# Enhanced model\n",
    "eb.compile(loss=\"categorical_crossentropy\",\n",
    "           optimizer=opt,\n",
    "           metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "# Enhanced model\n",
    "eb_history = eb.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and training history\n",
    "# Enhanced model\n",
    "eb.save(\"models/eb.h5\")\n",
    "with open(\"models/eb_history\", \"wb\") as f:\n",
    "    pickle.dump(eb_history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced model with batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "# Enhanced model with batch normalization\n",
    "ebbn = Sequential()\n",
    "ebbn.add(Conv2D(filters=16, kernel_size=(5,5), padding=\"same\", input_shape=(32,32,3),\n",
    "                activation=\"relu\"))\n",
    "ebbn.add(Conv2D(filters=16, kernel_size=(5,5), padding=\"same\",\n",
    "                activation=\"relu\"))\n",
    "ebbn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "ebbn.add(BatchNormalization())\n",
    "ebbn.add(Conv2D(filters=32, kernel_size=(5,5), padding=\"same\",\n",
    "                activation=\"relu\"))\n",
    "ebbn.add(Conv2D(filters=32, kernel_size=(5,5), padding=\"same\",\n",
    "                activation=\"relu\"))\n",
    "ebbn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "ebbn.add(BatchNormalization())\n",
    "ebbn.add(Conv2D(filters=64, kernel_size=(5,5), padding=\"same\",\n",
    "                activation=\"relu\"))\n",
    "ebbn.add(Conv2D(filters=64, kernel_size=(5,5), padding=\"same\",\n",
    "                activation=\"relu\"))\n",
    "ebbn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "ebbn.add(Flatten())\n",
    "ebbn.add(BatchNormalization())\n",
    "ebbn.add(Dense(128, activation=\"relu\"))\n",
    "ebbn.add(BatchNormalization())\n",
    "ebbn.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "# Enhanced model with batch normalization\n",
    "ebbn.compile(loss=\"categorical_crossentropy\",\n",
    "             optimizer=opt,\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "# Enhanced model with batch normalization\n",
    "ebbn_history = ebbn.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and training history\n",
    "# Enhanced model with batch normalization\n",
    "ebbn.save(\"models/ebbn.h5\")\n",
    "with open(\"models/ebbn_history\", \"wb\") as f:\n",
    "    pickle.dump(ebbn_history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced model with batch normalization and dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "# Enchanced model with batch normalization and dropout\n",
    "ebbnd = Sequential()\n",
    "ebbnd.add(Conv2D(filters=16, kernel_size=(5,5), padding=\"same\", input_shape=(32,32,3),\n",
    "                 activation=\"relu\"))\n",
    "ebbnd.add(Conv2D(filters=16, kernel_size=(5,5), padding=\"same\",\n",
    "                 activation=\"relu\"))\n",
    "ebbnd.add(MaxPooling2D(pool_size=(2,2)))\n",
    "ebbnd.add(Dropout(0.2))\n",
    "\n",
    "ebbnd.add(BatchNormalization())\n",
    "ebbnd.add(Conv2D(filters=32, kernel_size=(5,5), padding=\"same\",\n",
    "                 activation=\"relu\"))\n",
    "ebbnd.add(Conv2D(filters=32, kernel_size=(5,5), padding=\"same\",\n",
    "                 activation=\"relu\"))\n",
    "ebbnd.add(MaxPooling2D(pool_size=(2,2)))\n",
    "ebbnd.add(Dropout(0.3))\n",
    "\n",
    "ebbnd.add(BatchNormalization())\n",
    "ebbnd.add(Conv2D(filters=64, kernel_size=(5,5), padding=\"same\",\n",
    "                 activation=\"relu\"))\n",
    "ebbnd.add(Conv2D(filters=64, kernel_size=(5,5), padding=\"same\",\n",
    "                 activation=\"relu\"))\n",
    "ebbnd.add(MaxPooling2D(pool_size=(2,2)))\n",
    "ebbnd.add(Dropout(0.4))\n",
    "\n",
    "ebbnd.add(Flatten())\n",
    "ebbnd.add(BatchNormalization())\n",
    "ebbnd.add(Dense(128, activation=\"relu\"))\n",
    "ebbnd.add(Dropout(0.5))\n",
    "ebbnd.add(BatchNormalization())\n",
    "ebbnd.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "# Enhanced model with batch normalization and dropout\n",
    "ebbnd.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=opt,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "# Enhanced model with batch normalization and dropout\n",
    "ebbnd_history = ebbnd.fit(x_train, y_train,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs,\n",
    "                          validation_data=(x_test, y_test),\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and training history\n",
    "# Enhanced model with batch normalization and dropout\n",
    "ebbnd.save(\"models/ebbnd.h5\")\n",
    "with open(\"models/ebbnd_history\", \"wb\") as f:\n",
    "    pickle.dump(ebbnd_history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced model with batch normalization and dropout, trained with augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit a image data generator on the training images\n",
    "# Used for data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.15,\n",
    "            height_shift_range=0.15,\n",
    "            horizontal_flip=True\n",
    ")\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "# Enhanced model with batch normalization and dropout (data augmentation)\n",
    "ebbndda = Sequential()\n",
    "ebbndda.add(Conv2D(filters=16, kernel_size=(5,5), padding=\"same\", input_shape=(32,32,3),\n",
    "                   activation=\"relu\"))\n",
    "ebbndda.add(Conv2D(filters=16, kernel_size=(5,5), padding=\"same\",\n",
    "                   activation=\"relu\"))\n",
    "ebbndda.add(MaxPooling2D(pool_size=(2,2)))\n",
    "ebbndda.add(Dropout(0.2))\n",
    "\n",
    "ebbndda.add(BatchNormalization())\n",
    "ebbndda.add(Conv2D(filters=32, kernel_size=(5,5), padding=\"same\",\n",
    "                 activation=\"relu\"))\n",
    "ebbndda.add(Conv2D(filters=32, kernel_size=(5,5), padding=\"same\",\n",
    "                 activation=\"relu\"))\n",
    "ebbndda.add(MaxPooling2D(pool_size=(2,2)))\n",
    "ebbndda.add(Dropout(0.3))\n",
    "\n",
    "ebbndda.add(BatchNormalization())\n",
    "ebbndda.add(Conv2D(filters=64, kernel_size=(5,5), padding=\"same\",\n",
    "                 activation=\"relu\"))\n",
    "ebbndda.add(Conv2D(filters=64, kernel_size=(5,5), padding=\"same\",\n",
    "                 activation=\"relu\"))\n",
    "ebbndda.add(MaxPooling2D(pool_size=(2,2)))\n",
    "ebbndda.add(Dropout(0.4))\n",
    "\n",
    "ebbndda.add(Flatten())\n",
    "ebbndda.add(BatchNormalization())\n",
    "ebbndda.add(Dense(128, activation=\"relu\"))\n",
    "ebbndda.add(Dropout(0.5))\n",
    "ebbndda.add(BatchNormalization())\n",
    "ebbndda.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "# Enhanced model with batch normalization and dropout (data augmentation)\n",
    "ebbndda.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=opt,\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "# Enhanced base model with batch normalization and dropout\n",
    "ebbndda_history = ebbndda.fit_generator(\n",
    "                    datagen.flow(x_train, y_train, batch_size = batch_size),\n",
    "                    epochs=epochs,\n",
    "                    validation_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and training history\n",
    "# Enhanced model with batch normalization and dropout (data augmentation)\n",
    "ebbndda.save(\"models/ebbndda.h5\")\n",
    "with open(\"models/ebbndda_history\", \"wb\") as f:\n",
    "    pickle.dump(ebbndda_history.history, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
